{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, r2_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_training_file = \"project_data_files/book_rating_train.csv\"\n",
    "book_testing_file = \"project_data_files/book_rating_test.csv\"\n",
    "\n",
    "train_names_file = \"project_data_files/book_text_features_doc2vec/train_name_doc2vec100.csv\"\n",
    "train_authors_file = \"project_data_files/book_text_features_doc2vec/train_authors_doc2vec20.csv\"\n",
    "train_desc_file = \"project_data_files/book_text_features_doc2vec/train_desc_doc2vec100.csv\"\n",
    "\n",
    "test_names_file = \"project_data_files/book_text_features_doc2vec/test_name_doc2vec100.csv\"\n",
    "test_authors_file = \"project_data_files/book_text_features_doc2vec/test_authors_doc2vec20.csv\"\n",
    "test_desc_file = \"project_data_files/book_text_features_doc2vec/test_desc_doc2vec100.csv\"\n",
    "\n",
    "train_data = pd.read_csv(book_training_file)\n",
    "test_data = pd.read_csv(book_testing_file)\n",
    "\n",
    "word_training_files = [train_names_file, train_authors_file, train_desc_file]\n",
    "word_testing_files = [test_names_file, test_authors_file, test_desc_file]\n",
    "\n",
    "word_train_data = [pd.read_csv(filename, index_col = False, delimiter = ',', header=None) for filename in word_training_files]\n",
    "word_test_data = [pd.read_csv(filename, index_col = False, delimiter = ',', header=None) for filename in word_testing_files]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_labels, predicted_labels):\n",
    "    confusion = confusion_matrix(true_labels, predicted_labels)\n",
    "    f1_m = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
    "    f1_w = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    r2 = r2_score(true_labels, predicted_labels)\n",
    "\n",
    "    print(\"Confusion Matrix :\\n\", confusion)\n",
    "    print(\"Accuracy : \", accuracy)\n",
    "    print(\"R2 Score : \", r2)\n",
    "    print(\"Micro F1 Score : \", f1_m)\n",
    "    print(\"Weighted F1 Score : \", f1_w)\n",
    "    return [confusion, f1_m, f1_w, accuracy, r2]\n",
    "\n",
    "def evaluate_kfold(label_set):\n",
    "    true_labels = [t_labels.tolist() for (t_labels, p_labels) in label_set]\n",
    "    t_labels = []\n",
    "    for x in true_labels:\n",
    "        t_labels.extend(x)\n",
    "    predicted_labels = [p_labels.tolist() for (t_labels, p_labels) in label_set]\n",
    "    p_labels = []\n",
    "    for x in predicted_labels:\n",
    "        p_labels.extend(x)\n",
    "    return evaluate(true_labels=t_labels, predicted_labels=p_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline: 0R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_R(labels):\n",
    "    ratings, rating_counts = np.unique(labels, return_counts=True)\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    probs = [(rating_counts[i] / num_labels, ratings[i]) for i in range(len(ratings))]\n",
    "    predicted_label = max(probs)[1]\n",
    "    predicted_labels = [predicted_label] * len(labels)\n",
    "    return predicted_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[    0  5864     0]\n",
      " [    0 16208     0]\n",
      " [    0   991     0]]\n",
      "Accuracy :  0.7027706716385552\n",
      "R2 Score :  -0.1767472937401695\n",
      "Micro F1 Score :  0.7027706716385552\n",
      "Weighted F1 Score :  0.5800976316323855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[    0,  5864,     0],\n",
       "        [    0, 16208,     0],\n",
       "        [    0,   991,     0]], dtype=int64),\n",
       " 0.7027706716385552,\n",
       " 0.5800976316323855,\n",
       " 0.7027706716385552,\n",
       " -0.1767472937401695]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 -R Baseline\n",
    "evaluate(train_data['rating_label'], zero_R(train_data['rating_label']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected Features and Train-Test Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features = train_data.columns[:-1]\n",
    "# label = train_data.columns[-1]\n",
    "\n",
    "# feat_train, feat_valid, label_train, label_valid = train_test_split(train_data[selected_features], train_data[label], test_size=0.2, random_state=1169800)\n",
    "\n",
    "# text_features = [\"Name\", \"Authors\", \"Description\"]\n",
    "\n",
    "# vec = CountVectorizer()\n",
    "# feat_train_transformed_m = [vec.fit_transform(feat_train[f]) for f in text_features]\n",
    "# feat_valid_fitted_m = [vec.transform(feat_valid[f]) for f in text_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM = [SVC()] * len(text_features)\n",
    "# for feat in range(len(text_features)):\n",
    "#     SVM[feat].fit(feat_train_transformed_m[feat].toarray(), label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted = []\n",
    "# for SVM_feat in range(len(SVM)):\n",
    "#     predicted.append(SVM_feat.predict(feat_valid_fitted_m[SVM_feat]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluated = []\n",
    "# for i in range(len(predicted)):\n",
    "#     evaluated.append(evaluate(label_valid, predicted[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Fold Cross Validation on SVM with all 4 datasets combined (Train, Names, Author, Description):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unknown is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m     y_pred \u001b[39m=\u001b[39m CombinedSVM\u001b[39m.\u001b[39mpredict(X_validate)\n\u001b[0;32m     29\u001b[0m     combined_evaluation\u001b[39m.\u001b[39mappend((y_validate, y_pred))\n\u001b[1;32m---> 31\u001b[0m evaluate_kfold(combined_evaluation)\n",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m, in \u001b[0;36mevaluate_kfold\u001b[1;34m(label_set)\u001b[0m\n\u001b[0;32m     16\u001b[0m true_labels \u001b[39m=\u001b[39m [t_labels \u001b[39mfor\u001b[39;00m (t_labels, p_labels) \u001b[39min\u001b[39;00m label_set]\n\u001b[0;32m     17\u001b[0m predicted_labels \u001b[39m=\u001b[39m [p_labels \u001b[39mfor\u001b[39;00m (t_labels, p_labels) \u001b[39min\u001b[39;00m label_set]\n\u001b[1;32m---> 18\u001b[0m \u001b[39mreturn\u001b[39;00m evaluate(true_labels\u001b[39m=\u001b[39;49mtrue_labels, predicted_labels\u001b[39m=\u001b[39;49mpredicted_labels)\n",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(true_labels, predicted_labels)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate\u001b[39m(true_labels, predicted_labels):\n\u001b[1;32m----> 2\u001b[0m     confusion \u001b[39m=\u001b[39m confusion_matrix(true_labels, predicted_labels)\n\u001b[0;32m      3\u001b[0m     f1_m \u001b[39m=\u001b[39m f1_score(true_labels, predicted_labels, average\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmicro\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     f1_w \u001b[39m=\u001b[39m f1_score(true_labels, predicted_labels, average\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:317\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfusion_matrix\u001b[39m(\n\u001b[0;32m    233\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, normalize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m    234\u001b[0m ):\n\u001b[0;32m    235\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \n\u001b[0;32m    237\u001b[0m \u001b[39m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    318\u001b[0m     \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    319\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:106\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmultilabel-indicator\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[0;32m    108\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    109\u001b[0m     y_true \u001b[39m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[1;31mValueError\u001b[0m: unknown is not supported"
     ]
    }
   ],
   "source": [
    "# Select useful features and remove unnecessary features\n",
    "selected_features = train_data.columns[:-1]\n",
    "label = train_data.columns[-1]\n",
    "text_features = [\"Name\", \"Authors\", \"Description\"]\n",
    "drop = [\"Publisher\", \"Language\"]\n",
    "\n",
    "# Add names, authors and descriptions datasets\n",
    "all_data = train_data[selected_features]\n",
    "for f in text_features:\n",
    "    all_data = all_data.drop(f, axis=1)\n",
    "\n",
    "for f in drop:\n",
    "    all_data = all_data.drop(f, axis=1)\n",
    "    \n",
    "for i in range(len(word_training_files)):\n",
    "    new_column_names = {x:text_features[i] + str(x) for x in word_train_data[i].columns}\n",
    "    all_data = all_data.join(word_train_data[i].rename(columns=new_column_names))\n",
    "\n",
    "# Use cross validation\n",
    "\n",
    "CombinedSVM = SVC()\n",
    "combined_evaluation = []\n",
    "k_folds = KFold(n_splits=10)\n",
    "for _, (train_index, validate_index) in enumerate(k_folds.split(all_data)):\n",
    "    X_train, X_validate = all_data.iloc[train_index], all_data.iloc[validate_index]\n",
    "    y_train, y_validate = train_data[label].iloc[train_index], train_data[label].iloc[validate_index]\n",
    "    CombinedSVM.fit(X_train, y_train)\n",
    "    y_pred = CombinedSVM.predict(X_validate)\n",
    "    combined_evaluation.append((y_validate, y_pred))\n",
    "\n",
    "evaluate_kfold(combined_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data2 = test_data\n",
    "for f in text_features:\n",
    "    test_data2 = test_data2.drop(f, axis=1)\n",
    "\n",
    "for f in drop:\n",
    "    test_data2 = test_data2.drop(f, axis=1)\n",
    "    \n",
    "for i in range(len(word_testing_files)):\n",
    "    new_column_names = {x:text_features[i] + str(x) for x in word_test_data[i].columns}\n",
    "    test_data2 = test_data2.join(word_test_data[i].rename(columns=new_column_names))\n",
    "    \n",
    "predictions = CombinedSVM.predict(test_data2)\n",
    "predict_data = pd.DataFrame({'id':test_data2.index+1, 'rating_label':predictions})\n",
    "predict_data.to_csv(\"1169800 CombinedSVM.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaging 3 SVM's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross validation\n",
    "AverageSVM = [SVC(), SVC(), SVC()]\n",
    "avg_evaluation = {}\n",
    "summary = []\n",
    "i=0\n",
    "k_folds = KFold(n_splits=10)\n",
    "for data in word_train_data:\n",
    "    for _, (train_index, validate_index) in enumerate(k_folds.split(data)):\n",
    "        X_train, X_validate = data.iloc[train_index], data.iloc[validate_index]\n",
    "        y_train, y_validate = train_data[label].iloc[train_index], train_data[label].iloc[validate_index]\n",
    "        AverageSVM[i].fit(X_train, y_train)\n",
    "        y_pred = AverageSVM[i].predict(X_validate)\n",
    "        if i in avg_evaluation:\n",
    "            avg_evaluation[i].append((y_validate, y_pred))\n",
    "        else:\n",
    "            avg_evaluation[i] = [(y_validate, y_pred)]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_avg_SVM = []\n",
    "i=0\n",
    "for data in word_test_data:\n",
    "    predict_avg_SVM.append(AverageSVM[i].predict(data))\n",
    "    i+=1\n",
    "\n",
    "avg_predicted_dataset = pd.DataFrame({index:predict_avg_SVM[index] for index in range(len(predict_avg_SVM))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[   36  5828     0]\n",
      " [   17 16191     0]\n",
      " [    0   991     0]]\n",
      "Accuracy :  0.7035945020162164\n",
      "R2 Score :  -0.17348570386692908\n",
      "Micro F1 Score :  0.7035945020162164\n",
      "Weighted F1 Score :  0.5833662411730984\n",
      "Confusion Matrix :\n",
      " [[   11  5853     0]\n",
      " [   16 16192     0]\n",
      " [    2   989     0]]\n",
      "Accuracy :  0.7025538741707497\n",
      "R2 Score :  -0.1786355826141508\n",
      "Micro F1 Score :  0.7025538741707497\n",
      "Weighted F1 Score :  0.5809024644274099\n",
      "Confusion Matrix :\n",
      " [[   72  5792     0]\n",
      " [   30 16178     0]\n",
      " [    0   991     0]]\n",
      "Accuracy :  0.7045917703681222\n",
      "R2 Score :  -0.16953746349405918\n",
      "Micro F1 Score :  0.7045917703681222\n",
      "Weighted F1 Score :  0.5866687613763812\n"
     ]
    }
   ],
   "source": [
    "summary_dict = {}\n",
    "for index in avg_evaluation.keys():\n",
    "    summary_dict[index] = evaluate_kfold(avg_evaluation[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_avg_values = []\n",
    "for instance in avg_predicted_dataset.iterrows():\n",
    "    ratings, counts = np.unique([instance[1][i] for i in range(len(AverageSVM))], return_counts=True)\n",
    "    val = sorted([(counts[i], ratings[i]) for i in range(len(ratings))])\n",
    "    final_avg_values.append(val[0][1])\n",
    "\n",
    "avg_SVM_predicted = pd.DataFrame({'id':avg_predicted_dataset.index+1, 'rating_label':final_avg_values})\n",
    "avg_SVM_predicted.to_csv(\"1169800 AvgSVM.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[   7 1138    0]\n",
      " [  16 3235    0]\n",
      " [   0  217    0]]\n",
      "Accuracy :  0.7027964448298287\n",
      "R2 Score :  -0.16649710184328903\n",
      "Micro F1 Score :  0.7027964448298287\n",
      "Weighted F1 Score :  0.5844973961111398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[   7, 1138,    0],\n",
       "        [  16, 3235,    0],\n",
       "        [   0,  217,    0]], dtype=int64),\n",
       " 0.7027964448298287,\n",
       " 0.5844973961111398,\n",
       " 0.7027964448298287,\n",
       " -0.16649710184328903]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(all_data, train_data[label], test_size=0.2)\n",
    "RFC = RandomForestClassifier()\n",
    "RFC.fit(X_train, y_train)\n",
    "RFC_summary = evaluate(y_validate, RFC.predict(X_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_predict = RFC.predict(test_data2)\n",
    "RFC_predict_data = pd.DataFrame({'id':test_data2.index+1, 'rating_label':RFC_predict})\n",
    "RFC_predict_data.to_csv(\"1169800 RFC.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
