{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, r2_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_training_file = \"project_data_files/book_rating_train.csv\"\n",
    "book_testing_file = \"project_data_files/book_rating_test.csv\"\n",
    "\n",
    "train_names_file = \"project_data_files/book_text_features_doc2vec/train_name_doc2vec100.csv\"\n",
    "train_authors_file = \"project_data_files/book_text_features_doc2vec/train_authors_doc2vec20.csv\"\n",
    "train_desc_file = \"project_data_files/book_text_features_doc2vec/train_desc_doc2vec100.csv\"\n",
    "\n",
    "test_names_file = \"project_data_files/book_text_features_doc2vec/test_name_doc2vec100.csv\"\n",
    "test_authors_file = \"project_data_files/book_text_features_doc2vec/test_authors_doc2vec20.csv\"\n",
    "test_desc_file = \"project_data_files/book_text_features_doc2vec/test_desc_doc2vec100.csv\"\n",
    "\n",
    "train_data = pd.read_csv(book_training_file)\n",
    "test_data = pd.read_csv(book_testing_file)\n",
    "\n",
    "word_training_files = [train_names_file, train_authors_file, train_desc_file]\n",
    "word_testing_files = [test_names_file, test_authors_file, test_desc_file]\n",
    "\n",
    "word_train_data = [pd.read_csv(filename, index_col = False, delimiter = ',', header=None) for filename in word_training_files]\n",
    "word_test_data = [pd.read_csv(filename, index_col = False, delimiter = ',', header=None) for filename in word_testing_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"all_data\" Dataset:\n",
    "# Select useful features and remove unnecessary features\n",
    "selected_features = train_data.columns[:-1]\n",
    "label = train_data.columns[-1]\n",
    "text_features = [\"Name\", \"Authors\", \"Description\"]\n",
    "drop = [\"Publisher\", \"Language\"]\n",
    "\n",
    "# Add names, authors and descriptions datasets\n",
    "all_data = train_data[selected_features]\n",
    "for f in text_features:\n",
    "    all_data = all_data.drop(f, axis=1)\n",
    "\n",
    "for f in drop:\n",
    "    all_data = all_data.drop(f, axis=1)\n",
    "    \n",
    "for i in range(len(word_training_files)):\n",
    "    new_column_names = {x:text_features[i] + str(x) for x in word_train_data[i].columns}\n",
    "    all_data = all_data.join(word_train_data[i].rename(columns=new_column_names))\n",
    "\n",
    "#\"test2\" dataset corresponding to \"all_data\"\n",
    "test_data2 = test_data\n",
    "for f in text_features:\n",
    "    test_data2 = test_data2.drop(f, axis=1)\n",
    "\n",
    "for f in drop:\n",
    "    test_data2 = test_data2.drop(f, axis=1)\n",
    "    \n",
    "for i in range(len(word_testing_files)):\n",
    "    new_column_names = {x:text_features[i] + str(x) for x in word_test_data[i].columns}\n",
    "    test_data2 = test_data2.join(word_test_data[i].rename(columns=new_column_names))\n",
    "\n",
    "\n",
    "#\"strings_data\" Dataset:\n",
    "new_column_names = {x:text_features[0] + str(x) for x in word_train_data[0].columns}\n",
    "strings_data = word_train_data[0].rename(columns=new_column_names)\n",
    "for i in range(len(word_testing_files)-1):\n",
    "    new_column_names = {x:text_features[i+1] + str(x) for x in word_train_data[i+1].columns}\n",
    "    strings_data = strings_data.join(word_train_data[i+1].rename(columns=new_column_names))\n",
    "\n",
    "#\"strings_data_test\" dataset corresponding to \"strings_data\"\n",
    "new_column_names = {x:text_features[0] + str(x) for x in word_test_data[0].columns}\n",
    "strings_data_test = word_test_data[0].rename(columns=new_column_names)\n",
    "for i in range(len(word_testing_files)-1):\n",
    "    new_column_names = {x:text_features[i+1] + str(x) for x in word_test_data[i+1].columns}\n",
    "    strings_data_test = strings_data_test.join(word_test_data[i+1].rename(columns=new_column_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_labels, predicted_labels):\n",
    "    confusion = confusion_matrix(true_labels, predicted_labels)\n",
    "    f1_m = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
    "    f1_w = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    r2 = r2_score(true_labels, predicted_labels)\n",
    "\n",
    "    print(\"Confusion Matrix :\\n\", confusion)\n",
    "    print(\"Accuracy : \", accuracy)\n",
    "    print(\"R2 Score : \", r2)\n",
    "    print(\"Micro F1 Score : \", f1_m)\n",
    "    print(\"Weighted F1 Score : \", f1_w)\n",
    "    return [confusion, f1_m, f1_w, accuracy, r2]\n",
    "\n",
    "def evaluate_kfold(label_set):\n",
    "    true_labels = [t_labels.tolist() for (t_labels, p_labels) in label_set]\n",
    "    t_labels = []\n",
    "    for x in true_labels:\n",
    "        t_labels.extend(x)\n",
    "    predicted_labels = [p_labels.tolist() for (t_labels, p_labels) in label_set]\n",
    "    p_labels = []\n",
    "    for x in predicted_labels:\n",
    "        p_labels.extend(x)\n",
    "    return evaluate(true_labels=t_labels, predicted_labels=p_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline: 0R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_R(labels):\n",
    "    ratings, rating_counts = np.unique(labels, return_counts=True)\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    probs = [(rating_counts[i] / num_labels, ratings[i]) for i in range(len(ratings))]\n",
    "    predicted_label = max(probs)[1]\n",
    "    predicted_labels = [predicted_label] * len(labels)\n",
    "    return predicted_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[    0  5864     0]\n",
      " [    0 16208     0]\n",
      " [    0   991     0]]\n",
      "Accuracy :  0.7027706716385552\n",
      "R2 Score :  -0.1767472937401695\n",
      "Micro F1 Score :  0.7027706716385552\n",
      "Weighted F1 Score :  0.5800976316323855\n"
     ]
    }
   ],
   "source": [
    "# 0 -R Baseline\n",
    "_0R = evaluate(train_data['rating_label'], zero_R(train_data['rating_label']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Fold Cross Validation on SVM with all 4 datasets combined (Train, Names, Author, Description):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[    0  5864     0]\n",
      " [    0 16205     3]\n",
      " [    0   990     1]]\n",
      "Accuracy :  0.702683952651433\n",
      "R2 Score :  -0.17709061898998435\n",
      "Micro F1 Score :  0.702683952651433\n",
      "Weighted F1 Score :  0.580135711234597\n"
     ]
    }
   ],
   "source": [
    "# Use cross validation\n",
    "CombinedSVM = SVC()\n",
    "combined_evaluation = []\n",
    "k_folds = KFold(n_splits=10)\n",
    "for _, (train_index, validate_index) in enumerate(k_folds.split(all_data)):\n",
    "    X_train, X_validate = all_data.iloc[train_index], all_data.iloc[validate_index]\n",
    "    y_train, y_validate = train_data[label].iloc[train_index], train_data[label].iloc[validate_index]\n",
    "    CombinedSVM.fit(X_train, y_train)\n",
    "    y_pred = CombinedSVM.predict(X_validate)\n",
    "    combined_evaluation.append((y_validate, y_pred))\n",
    "\n",
    "combined = evaluate_kfold(combined_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = CombinedSVM.predict(test_data2)\n",
    "predict_data = pd.DataFrame({'id':test_data2.index+1, 'rating_label':predictions})\n",
    "predict_data.to_csv(\"1169800 CombinedSVM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[   71  5793     0]\n",
      " [   30 16178     0]\n",
      " [    0   991     0]]\n",
      "Accuracy :  0.704548410874561\n",
      "R2 Score :  -0.1697091261189665\n",
      "Micro F1 Score :  0.704548410874561\n",
      "Weighted F1 Score :  0.5865697187363557\n"
     ]
    }
   ],
   "source": [
    "# Use cross validation\n",
    "StringsSVM = SVC()\n",
    "strings_evaluation = []\n",
    "k_folds = KFold(n_splits=10)\n",
    "for _, (train_index, validate_index) in enumerate(k_folds.split(strings_data)):\n",
    "    X_train, X_validate = strings_data.iloc[train_index], strings_data.iloc[validate_index]\n",
    "    y_train, y_validate = train_data[label].iloc[train_index], train_data[label].iloc[validate_index]\n",
    "    StringsSVM.fit(X_train, y_train)\n",
    "    y_pred = StringsSVM.predict(X_validate)\n",
    "    strings_evaluation.append((y_validate, y_pred))\n",
    "\n",
    "strings_only = evaluate_kfold(strings_evaluation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaging 3 SVM's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross validation\n",
    "AverageSVM = [SVC(), SVC(), SVC()]\n",
    "avg_evaluation = {}\n",
    "summary = []\n",
    "i=0\n",
    "k_folds = KFold(n_splits=10)\n",
    "for data in word_train_data:\n",
    "    for _, (train_index, validate_index) in enumerate(k_folds.split(data)):\n",
    "        X_train, X_validate = data.iloc[train_index], data.iloc[validate_index]\n",
    "        y_train, y_validate = train_data[label].iloc[train_index], train_data[label].iloc[validate_index]\n",
    "        AverageSVM[i].fit(X_train, y_train)\n",
    "        y_pred = AverageSVM[i].predict(X_validate)\n",
    "        if i in avg_evaluation:\n",
    "            avg_evaluation[i].append((y_validate, y_pred))\n",
    "        else:\n",
    "            avg_evaluation[i] = [(y_validate, y_pred)]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_avg_SVM = []\n",
    "i=0\n",
    "for data in word_test_data:\n",
    "    predict_avg_SVM.append(AverageSVM[i].predict(data))\n",
    "    i+=1\n",
    "\n",
    "avg_predicted_dataset = pd.DataFrame({index:predict_avg_SVM[index] for index in range(len(predict_avg_SVM))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[   36  5828     0]\n",
      " [   17 16191     0]\n",
      " [    0   991     0]]\n",
      "Accuracy :  0.7035945020162164\n",
      "R2 Score :  -0.17348570386692908\n",
      "Micro F1 Score :  0.7035945020162164\n",
      "Weighted F1 Score :  0.5833662411730984\n",
      "Confusion Matrix :\n",
      " [[   11  5853     0]\n",
      " [   16 16192     0]\n",
      " [    2   989     0]]\n",
      "Accuracy :  0.7025538741707497\n",
      "R2 Score :  -0.1786355826141508\n",
      "Micro F1 Score :  0.7025538741707497\n",
      "Weighted F1 Score :  0.5809024644274099\n",
      "Confusion Matrix :\n",
      " [[   72  5792     0]\n",
      " [   30 16178     0]\n",
      " [    0   991     0]]\n",
      "Accuracy :  0.7045917703681222\n",
      "R2 Score :  -0.16953746349405918\n",
      "Micro F1 Score :  0.7045917703681222\n",
      "Weighted F1 Score :  0.5866687613763812\n"
     ]
    }
   ],
   "source": [
    "summary_dict = {}\n",
    "for index in avg_evaluation.keys():\n",
    "    summary_dict[AverageSVM[index]] = evaluate_kfold(avg_evaluation[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_avg_values = []\n",
    "for instance in avg_predicted_dataset.iterrows():\n",
    "    ratings, counts = np.unique([instance[1][i] for i in range(len(AverageSVM))], return_counts=True)\n",
    "    val = sorted([(counts[i], ratings[i]) for i in range(len(ratings))])\n",
    "    final_avg_values.append(val[0][1])\n",
    "\n",
    "avg_SVM_predicted = pd.DataFrame({'id':avg_predicted_dataset.index+1, 'rating_label':final_avg_values})\n",
    "avg_SVM_predicted.to_csv(\"1169800 AvgSVM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_accuracy_SVM = AverageSVM[2].predict(word_test_data[2])\n",
    "highest_accuracy_SVM_predicted = pd.DataFrame({'id':word_test_data[2].index+1, 'rating_label':highest_accuracy_SVM})\n",
    "highest_accuracy_SVM_predicted.to_csv(\"1169800 AccurateSVM.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[   7 1209    0]\n",
      " [  10 3213    0]\n",
      " [   0  174    0]]\n",
      "Accuracy :  0.6980273141122914\n",
      "R2 Score :  -0.20644758299654598\n",
      "Micro F1 Score :  0.6980273141122914\n",
      "Weighted F1 Score :  0.5771972499416257\n"
     ]
    }
   ],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(strings_data, train_data[label], test_size=0.2)\n",
    "RFC = RandomForestClassifier()\n",
    "RFC.fit(X_train, y_train)\n",
    "RFC_summary = evaluate(y_validate, RFC.predict(X_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_predict = RFC.predict(strings_data_test)\n",
    "RFC_predict_data = pd.DataFrame({'id':strings_data_test.index+1, 'rating_label':RFC_predict})\n",
    "RFC_predict_data.to_csv(\"1169800 RFC.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use all for Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SamSike\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[  14 1396    0]\n",
      " [   5 4098    0]\n",
      " [   0  253    0]]\n",
      "Accuracy :  0.7131460284425946\n",
      "R2 Score :  -0.15596635188272212\n",
      "Micro F1 Score :  0.7131460284425946\n",
      "Weighted F1 Score :  0.5968881287313864\n"
     ]
    }
   ],
   "source": [
    "estimators = [('rfc', RandomForestClassifier()), ('svm', SVC())]\n",
    "X_train_stack, X_test_stack, y_train_stack, y_test_stack = train_test_split(strings_data, train_data[label])\n",
    "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=10, n_jobs=-1)\n",
    "stack.fit(X_train_stack, y_train_stack)\n",
    "stack_eval = evaluate(y_test_stack, stack.predict(X_test_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stacked_predict = stack.predict(strings_data_test)\n",
    "Stacked_predict_data = pd.DataFrame({'id':strings_data_test.index+1, 'rating_label':Stacked_predict})\n",
    "Stacked_predict_data.to_csv(\"1169800 Stack.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
