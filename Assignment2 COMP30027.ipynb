{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, r2_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_training_file = \"project_data_files/book_rating_train.csv\"\n",
    "book_testing_file = \"project_data_files/book_rating_test.csv\"\n",
    "\n",
    "train_names_file = \"project_data_files/book_text_features_doc2vec/train_name_doc2vec100.csv\"\n",
    "train_authors_file = \"project_data_files/book_text_features_doc2vec/train_authors_doc2vec20.csv\"\n",
    "train_desc_file = \"project_data_files/book_text_features_doc2vec/train_desc_doc2vec100.csv\"\n",
    "\n",
    "test_names_file = \"project_data_files/book_text_features_doc2vec/test_name_doc2vec100.csv\"\n",
    "test_authors_file = \"project_data_files/book_text_features_doc2vec/test_authors_doc2vec20.csv\"\n",
    "test_desc_file = \"project_data_files/book_text_features_doc2vec/test_desc_doc2vec100.csv\"\n",
    "\n",
    "train_data = pd.read_csv(book_training_file)\n",
    "test_data = pd.read_csv(book_testing_file)\n",
    "\n",
    "word_training_files = [train_names_file, train_authors_file, train_desc_file]\n",
    "word_testing_files = [test_names_file, test_authors_file, test_desc_file]\n",
    "\n",
    "word_train_data = [pd.read_csv(filename, index_col = False, delimiter = ',', header=None) for filename in word_training_files]\n",
    "word_test_data = [pd.read_csv(filename, index_col = False, delimiter = ',', header=None) for filename in word_testing_files]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_labels, predicted_labels):\n",
    "    confusion = confusion_matrix(true_labels, predicted_labels)\n",
    "    f1_m = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
    "    f1_w = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    r2 = r2_score(true_labels, predicted_labels)\n",
    "\n",
    "    print(\"Confusion Matrix :\\n\", confusion)\n",
    "    print(\"Accuracy : \", accuracy)\n",
    "    print(\"R2 Score : \", r2)\n",
    "    print(\"Micro F1 Score : \", f1_m)\n",
    "    print(\"Weighted F1 Score : \", f1_w)\n",
    "    return [confusion, f1_m, f1_w, accuracy, r2]\n",
    "\n",
    "def evaluate_kfold(label_set):\n",
    "    true_labels = [t_labels.tolist() for (t_labels, p_labels) in label_set]\n",
    "    t_labels = []\n",
    "    for x in true_labels:\n",
    "        t_labels.extend(x)\n",
    "    predicted_labels = [p_labels.tolist() for (t_labels, p_labels) in label_set]\n",
    "    p_labels = []\n",
    "    for x in predicted_labels:\n",
    "        p_labels.extend(x)\n",
    "    return evaluate(true_labels=t_labels, predicted_labels=p_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline: 0R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_R(labels):\n",
    "    ratings, rating_counts = np.unique(labels, return_counts=True)\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    probs = [(rating_counts[i] / num_labels, ratings[i]) for i in range(len(ratings))]\n",
    "    predicted_label = max(probs)[1]\n",
    "    predicted_labels = [predicted_label] * len(labels)\n",
    "    return predicted_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[    0  5864     0]\n",
      " [    0 16208     0]\n",
      " [    0   991     0]]\n",
      "Accuracy :  0.7027706716385552\n",
      "R2 Score :  -0.1767472937401695\n",
      "Micro F1 Score :  0.7027706716385552\n",
      "Weighted F1 Score :  0.5800976316323855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[    0,  5864,     0],\n",
       "        [    0, 16208,     0],\n",
       "        [    0,   991,     0]], dtype=int64),\n",
       " 0.7027706716385552,\n",
       " 0.5800976316323855,\n",
       " 0.7027706716385552,\n",
       " -0.1767472937401695]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 -R Baseline\n",
    "evaluate(train_data['rating_label'], zero_R(train_data['rating_label']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected Features and Train-Test Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features = train_data.columns[:-1]\n",
    "# label = train_data.columns[-1]\n",
    "\n",
    "# feat_train, feat_valid, label_train, label_valid = train_test_split(train_data[selected_features], train_data[label], test_size=0.2, random_state=1169800)\n",
    "\n",
    "# text_features = [\"Name\", \"Authors\", \"Description\"]\n",
    "\n",
    "# vec = CountVectorizer()\n",
    "# feat_train_transformed_m = [vec.fit_transform(feat_train[f]) for f in text_features]\n",
    "# feat_valid_fitted_m = [vec.transform(feat_valid[f]) for f in text_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM = [SVC()] * len(text_features)\n",
    "# for feat in range(len(text_features)):\n",
    "#     SVM[feat].fit(feat_train_transformed_m[feat].toarray(), label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted = []\n",
    "# for SVM_feat in range(len(SVM)):\n",
    "#     predicted.append(SVM_feat.predict(feat_valid_fitted_m[SVM_feat]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluated = []\n",
    "# for i in range(len(predicted)):\n",
    "#     evaluated.append(evaluate(label_valid, predicted[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Fold Cross Validation on SVM with all 4 datasets combined (Train, Names, Author, Description):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select useful features and remove unnecessary features\n",
    "selected_features = train_data.columns[:-1]\n",
    "label = train_data.columns[-1]\n",
    "text_features = [\"Name\", \"Authors\", \"Description\"]\n",
    "drop = [\"Publisher\", \"Language\"]\n",
    "\n",
    "# Add names, authors and descriptions datasets\n",
    "all_data = train_data[selected_features]\n",
    "for f in text_features:\n",
    "    all_data = all_data.drop(f, axis=1)\n",
    "\n",
    "for f in drop:\n",
    "    all_data = all_data.drop(f, axis=1)\n",
    "    \n",
    "for i in range(len(word_training_files)):\n",
    "    new_column_names = {x:text_features[i] + str(x) for x in word_train_data[i].columns}\n",
    "    all_data = all_data.join(word_train_data[i].rename(columns=new_column_names))\n",
    "\n",
    "# Use cross validation\n",
    "\n",
    "CombinedSVM = SVC()\n",
    "combined_evaluation = []\n",
    "k_folds = KFold(n_splits=10)\n",
    "for _, (train_index, validate_index) in enumerate(k_folds.split(all_data)):\n",
    "    X_train, X_validate = all_data.iloc[train_index], all_data.iloc[validate_index]\n",
    "    y_train, y_validate = train_data[label].iloc[train_index], train_data[label].iloc[validate_index]\n",
    "    CombinedSVM.fit(X_train, y_train)\n",
    "    y_pred = CombinedSVM.predict(X_validate)\n",
    "    combined_evaluation.append((y_validate, y_pred))\n",
    "\n",
    "evaluate_kfold(combined_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data2 = test_data\n",
    "for f in text_features:\n",
    "    test_data2 = test_data2.drop(f, axis=1)\n",
    "\n",
    "for f in drop:\n",
    "    test_data2 = test_data2.drop(f, axis=1)\n",
    "    \n",
    "for i in range(len(word_testing_files)):\n",
    "    new_column_names = {x:text_features[i] + str(x) for x in word_test_data[i].columns}\n",
    "    test_data2 = test_data2.join(word_test_data[i].rename(columns=new_column_names))\n",
    "    \n",
    "predictions = CombinedSVM.predict(test_data2)\n",
    "predict_data = pd.DataFrame({'id':test_data2.index+1, 'rating_label':predictions})\n",
    "predict_data.to_csv(\"1169800 CombinedSVM.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaging 3 SVM's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross validation\n",
    "AverageSVM = [SVC(), SVC(), SVC()]\n",
    "avg_evaluation = {}\n",
    "summary = []\n",
    "i=0\n",
    "k_folds = KFold(n_splits=10)\n",
    "for data in word_train_data:\n",
    "    for _, (train_index, validate_index) in enumerate(k_folds.split(data)):\n",
    "        X_train, X_validate = data.iloc[train_index], data.iloc[validate_index]\n",
    "        y_train, y_validate = train_data[label].iloc[train_index], train_data[label].iloc[validate_index]\n",
    "        AverageSVM[i].fit(X_train, y_train)\n",
    "        y_pred = AverageSVM[i].predict(X_validate)\n",
    "        if i in avg_evaluation:\n",
    "            avg_evaluation[i].append((y_validate, y_pred))\n",
    "        else:\n",
    "            avg_evaluation[i] = [(y_validate, y_pred)]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_avg_SVM = []\n",
    "i=0\n",
    "for data in word_test_data:\n",
    "    predict_avg_SVM.append(AverageSVM[i].predict(data))\n",
    "    i+=1\n",
    "\n",
    "avg_predicted_dataset = pd.DataFrame({index:predict_avg_SVM[index] for index in range(len(predict_avg_SVM))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0       4.0\n",
      "1       4.0\n",
      "2       4.0\n",
      "3       4.0\n",
      "4       3.0\n",
      "       ... \n",
      "2302    4.0\n",
      "2303    3.0\n",
      "2304    4.0\n",
      "2305    4.0\n",
      "2306    4.0\n",
      "Name: rating_label, Length: 2307, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (2307    4.0\n",
      "2308    4.0\n",
      "2309    4.0\n",
      "2310    4.0\n",
      "2311    3.0\n",
      "       ... \n",
      "4609    4.0\n",
      "4610    4.0\n",
      "4611    4.0\n",
      "4612    4.0\n",
      "4613    4.0\n",
      "Name: rating_label, Length: 2307, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (4614    3.0\n",
      "4615    4.0\n",
      "4616    4.0\n",
      "4617    4.0\n",
      "4618    4.0\n",
      "       ... \n",
      "6916    4.0\n",
      "6917    4.0\n",
      "6918    3.0\n",
      "6919    3.0\n",
      "6920    4.0\n",
      "Name: rating_label, Length: 2307, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (6921    4.0\n",
      "6922    3.0\n",
      "6923    4.0\n",
      "6924    3.0\n",
      "6925    4.0\n",
      "       ... \n",
      "9222    4.0\n",
      "9223    4.0\n",
      "9224    4.0\n",
      "9225    3.0\n",
      "9226    3.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (9227     5.0\n",
      "9228     4.0\n",
      "9229     3.0\n",
      "9230     4.0\n",
      "9231     3.0\n",
      "        ... \n",
      "11528    3.0\n",
      "11529    4.0\n",
      "11530    3.0\n",
      "11531    4.0\n",
      "11532    4.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (11533    3.0\n",
      "11534    4.0\n",
      "11535    3.0\n",
      "11536    5.0\n",
      "11537    4.0\n",
      "        ... \n",
      "13834    4.0\n",
      "13835    4.0\n",
      "13836    3.0\n",
      "13837    3.0\n",
      "13838    4.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (13839    3.0\n",
      "13840    3.0\n",
      "13841    4.0\n",
      "13842    4.0\n",
      "13843    4.0\n",
      "        ... \n",
      "16140    4.0\n",
      "16141    4.0\n",
      "16142    4.0\n",
      "16143    4.0\n",
      "16144    4.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (16145    4.0\n",
      "16146    4.0\n",
      "16147    4.0\n",
      "16148    4.0\n",
      "16149    4.0\n",
      "        ... \n",
      "18446    4.0\n",
      "18447    3.0\n",
      "18448    4.0\n",
      "18449    4.0\n",
      "18450    3.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (18451    3.0\n",
      "18452    4.0\n",
      "18453    3.0\n",
      "18454    3.0\n",
      "18455    4.0\n",
      "        ... \n",
      "20752    4.0\n",
      "20753    4.0\n",
      "20754    4.0\n",
      "20755    4.0\n",
      "20756    4.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (20757    3.0\n",
      "20758    4.0\n",
      "20759    4.0\n",
      "20760    4.0\n",
      "20761    4.0\n",
      "        ... \n",
      "23058    4.0\n",
      "23059    4.0\n",
      "23060    4.0\n",
      "23061    4.0\n",
      "23062    4.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (0       4.0\n",
      "1       4.0\n",
      "2       4.0\n",
      "3       4.0\n",
      "4       3.0\n",
      "       ... \n",
      "2302    4.0\n",
      "2303    3.0\n",
      "2304    4.0\n",
      "2305    4.0\n",
      "2306    4.0\n",
      "Name: rating_label, Length: 2307, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (2307    4.0\n",
      "2308    4.0\n",
      "2309    4.0\n",
      "2310    4.0\n",
      "2311    3.0\n",
      "       ... \n",
      "4609    4.0\n",
      "4610    4.0\n",
      "4611    4.0\n",
      "4612    4.0\n",
      "4613    4.0\n",
      "Name: rating_label, Length: 2307, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (4614    3.0\n",
      "4615    4.0\n",
      "4616    4.0\n",
      "4617    4.0\n",
      "4618    4.0\n",
      "       ... \n",
      "6916    4.0\n",
      "6917    4.0\n",
      "6918    3.0\n",
      "6919    3.0\n",
      "6920    4.0\n",
      "Name: rating_label, Length: 2307, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (6921    4.0\n",
      "6922    3.0\n",
      "6923    4.0\n",
      "6924    3.0\n",
      "6925    4.0\n",
      "       ... \n",
      "9222    4.0\n",
      "9223    4.0\n",
      "9224    4.0\n",
      "9225    3.0\n",
      "9226    3.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (9227     5.0\n",
      "9228     4.0\n",
      "9229     3.0\n",
      "9230     4.0\n",
      "9231     3.0\n",
      "        ... \n",
      "11528    3.0\n",
      "11529    4.0\n",
      "11530    3.0\n",
      "11531    4.0\n",
      "11532    4.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (11533    3.0\n",
      "11534    4.0\n",
      "11535    3.0\n",
      "11536    5.0\n",
      "11537    4.0\n",
      "        ... \n",
      "13834    4.0\n",
      "13835    4.0\n",
      "13836    3.0\n",
      "13837    3.0\n",
      "13838    4.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (13839    3.0\n",
      "13840    3.0\n",
      "13841    4.0\n",
      "13842    4.0\n",
      "13843    4.0\n",
      "        ... \n",
      "16140    4.0\n",
      "16141    4.0\n",
      "16142    4.0\n",
      "16143    4.0\n",
      "16144    4.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (16145    4.0\n",
      "16146    4.0\n",
      "16147    4.0\n",
      "16148    4.0\n",
      "16149    4.0\n",
      "        ... \n",
      "18446    4.0\n",
      "18447    3.0\n",
      "18448    4.0\n",
      "18449    4.0\n",
      "18450    3.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (18451    3.0\n",
      "18452    4.0\n",
      "18453    3.0\n",
      "18454    3.0\n",
      "18455    4.0\n",
      "        ... \n",
      "20752    4.0\n",
      "20753    4.0\n",
      "20754    4.0\n",
      "20755    4.0\n",
      "20756    4.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (20757    3.0\n",
      "20758    4.0\n",
      "20759    4.0\n",
      "20760    4.0\n",
      "20761    4.0\n",
      "        ... \n",
      "23058    4.0\n",
      "23059    4.0\n",
      "23060    4.0\n",
      "23061    4.0\n",
      "23062    4.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (0       4.0\n",
      "1       4.0\n",
      "2       4.0\n",
      "3       4.0\n",
      "4       3.0\n",
      "       ... \n",
      "2302    4.0\n",
      "2303    3.0\n",
      "2304    4.0\n",
      "2305    4.0\n",
      "2306    4.0\n",
      "Name: rating_label, Length: 2307, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (2307    4.0\n",
      "2308    4.0\n",
      "2309    4.0\n",
      "2310    4.0\n",
      "2311    3.0\n",
      "       ... \n",
      "4609    4.0\n",
      "4610    4.0\n",
      "4611    4.0\n",
      "4612    4.0\n",
      "4613    4.0\n",
      "Name: rating_label, Length: 2307, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (4614    3.0\n",
      "4615    4.0\n",
      "4616    4.0\n",
      "4617    4.0\n",
      "4618    4.0\n",
      "       ... \n",
      "6916    4.0\n",
      "6917    4.0\n",
      "6918    3.0\n",
      "6919    3.0\n",
      "6920    4.0\n",
      "Name: rating_label, Length: 2307, dtype: float64, array([4., 4., 4., ..., 4., 3., 4.])), (6921    4.0\n",
      "6922    3.0\n",
      "6923    4.0\n",
      "6924    3.0\n",
      "6925    4.0\n",
      "       ... \n",
      "9222    4.0\n",
      "9223    4.0\n",
      "9224    4.0\n",
      "9225    3.0\n",
      "9226    3.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (9227     5.0\n",
      "9228     4.0\n",
      "9229     3.0\n",
      "9230     4.0\n",
      "9231     3.0\n",
      "        ... \n",
      "11528    3.0\n",
      "11529    4.0\n",
      "11530    3.0\n",
      "11531    4.0\n",
      "11532    4.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (11533    3.0\n",
      "11534    4.0\n",
      "11535    3.0\n",
      "11536    5.0\n",
      "11537    4.0\n",
      "        ... \n",
      "13834    4.0\n",
      "13835    4.0\n",
      "13836    3.0\n",
      "13837    3.0\n",
      "13838    4.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (13839    3.0\n",
      "13840    3.0\n",
      "13841    4.0\n",
      "13842    4.0\n",
      "13843    4.0\n",
      "        ... \n",
      "16140    4.0\n",
      "16141    4.0\n",
      "16142    4.0\n",
      "16143    4.0\n",
      "16144    4.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (16145    4.0\n",
      "16146    4.0\n",
      "16147    4.0\n",
      "16148    4.0\n",
      "16149    4.0\n",
      "        ... \n",
      "18446    4.0\n",
      "18447    3.0\n",
      "18448    4.0\n",
      "18449    4.0\n",
      "18450    3.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (18451    3.0\n",
      "18452    4.0\n",
      "18453    3.0\n",
      "18454    3.0\n",
      "18455    4.0\n",
      "        ... \n",
      "20752    4.0\n",
      "20753    4.0\n",
      "20754    4.0\n",
      "20755    4.0\n",
      "20756    4.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.])), (20757    3.0\n",
      "20758    4.0\n",
      "20759    4.0\n",
      "20760    4.0\n",
      "20761    4.0\n",
      "        ... \n",
      "23058    4.0\n",
      "23059    4.0\n",
      "23060    4.0\n",
      "23061    4.0\n",
      "23062    4.0\n",
      "Name: rating_label, Length: 2306, dtype: float64, array([4., 4., 4., ..., 4., 4., 4.]))]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[235], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m summary_dict \u001b[39m=\u001b[39m {}\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(avg_evaluation)\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m avg_evaluation\u001b[39m.\u001b[39;49mkeys():\n\u001b[0;32m      4\u001b[0m     summary_dict[AverageSVM[index]] \u001b[39m=\u001b[39m evaluate_kfold(avg_evaluation[index])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "summary_dict = {}\n",
    "for index in avg_evaluation.keys():\n",
    "    summary_dict[AverageSVM[index]] = evaluate_kfold(avg_evaluation[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_avg_values = []\n",
    "for instance in avg_predicted_dataset.iterrows():\n",
    "    ratings, counts = np.unique([instance[1][i] for i in range(len(AverageSVM))], return_counts=True)\n",
    "    val = sorted([(counts[i], ratings[i]) for i in range(len(ratings))])\n",
    "    final_avg_values.append(val[0][1])\n",
    "\n",
    "avg_SVM_predicted = pd.DataFrame({'id':avg_predicted_dataset.index+1, 'rating_label':final_avg_values})\n",
    "avg_SVM_predicted.to_csv(\"1169800 AvgSVM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_accuracy = sorted([(accuracy, key()) for key, (_, _, _, accuracy, _) in summary_dict.items()], key=lambda x:x[0])[0]\n",
    "highest_accuracy_SVM = AverageSVM[2].predict(word_test_data[2])\n",
    "highest_accuracy_SVM_predicted = pd.DataFrame({'id':word_test_data[2].index+1, 'rating_label':highest_accuracy_SVM})\n",
    "highest_accuracy_SVM_predicted.to_csv(\"1169800 AccurateSVM.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[   7 1138    0]\n",
      " [  16 3235    0]\n",
      " [   0  217    0]]\n",
      "Accuracy :  0.7027964448298287\n",
      "R2 Score :  -0.16649710184328903\n",
      "Micro F1 Score :  0.7027964448298287\n",
      "Weighted F1 Score :  0.5844973961111398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[   7, 1138,    0],\n",
       "        [  16, 3235,    0],\n",
       "        [   0,  217,    0]], dtype=int64),\n",
       " 0.7027964448298287,\n",
       " 0.5844973961111398,\n",
       " 0.7027964448298287,\n",
       " -0.16649710184328903]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(all_data, train_data[label], test_size=0.2)\n",
    "RFC = RandomForestClassifier()\n",
    "RFC.fit(X_train, y_train)\n",
    "RFC_summary = evaluate(y_validate, RFC.predict(X_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_predict = RFC.predict(test_data2)\n",
    "RFC_predict_data = pd.DataFrame({'id':test_data2.index+1, 'rating_label':RFC_predict})\n",
    "RFC_predict_data.to_csv(\"1169800 RFC.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
